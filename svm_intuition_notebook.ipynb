{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2e95d9",
   "metadata": {},
   "source": [
    "# SVM Intuition: Dot Products, Distances, and Simple Linear SVMs\n",
    "\n",
    "This notebook builds geometric intuition for **dot products** and **distances**, then connects them to\n",
    "a **linear SVM** on a toy 2D dataset. It finishes with a small experiment on two digits\n",
    "(MNIST if available, otherwise the scikit-learn 8×8 digits as a fallback).\n",
    "\n",
    "**Conventions (ISLP-style):** decision function $f(x) = w^\\top x + b$; margin $\\frac{|w^\\top x + b|}{\\|w\\|}$;\n",
    "hinge loss $\\max(0, 1 - y\\,f(x))$ with labels $y\\in\\{-1,+1\\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cac608",
   "metadata": {},
   "source": [
    "## 1) Dot product intuition\n",
    "\n",
    "Think of $u\\cdot v$ as **how much of $u$ lies along $v$**, i.e., the length of the **projection** of $u$\n",
    "onto $v$ times $\\|v\\|$. If $u\\cdot v>0$, they broadly point in the same direction; if $<0$, in opposite directions.\n",
    "\n",
    "Run the next cell and examine the vectors, the projection $\\mathrm{proj}_v(u)$, and the residual $u-\\mathrm{proj}_v(u)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_vectors_and_projection(u, v, title_suffix=\"\"):\n",
    "    plt.figure()\n",
    "    plt.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1)\n",
    "    plt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1)\n",
    "    t = (u @ v) / (v @ v)\n",
    "    proj = t * v\n",
    "    r = u - proj\n",
    "    plt.quiver(0, 0, proj[0], proj[1], angles='xy', scale_units='xy', scale=1)\n",
    "    plt.quiver(proj[0], proj[1], r[0], r[1], angles='xy', scale_units='xy', scale=1)\n",
    "    plt.text(u[0]*1.03, u[1]*1.03, \"u\")\n",
    "    plt.text(v[0]*1.03, v[1]*1.03, \"v\")\n",
    "    plt.text(proj[0]*1.03, proj[1]*1.03, \"proj_v(u)\")\n",
    "    plt.axhline(0, linewidth=0.5); plt.axvline(0, linewidth=0.5)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.xlim(-5, 5); plt.ylim(-5, 5)\n",
    "    plt.grid(True, linewidth=0.3)\n",
    "    dp = float(u @ v)\n",
    "    plt.title(f\"Dot product & projection{title_suffix} — u·v = {dp:.2f}\")\n",
    "\n",
    "u1 = np.array([3.0, 1.5])\n",
    "v1 = np.array([2.0, 0.5])\n",
    "plot_vectors_and_projection(u1, v1, \" (positive)\")\n",
    "\n",
    "u2 = np.array([2.0, 1.0])\n",
    "v2 = np.array([-1.0, -2.0])\n",
    "plot_vectors_and_projection(u2, v2, \" (negative)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377d8cf",
   "metadata": {},
   "source": [
    "### EXERCISE A — Try your own vectors\n",
    "Pick new vectors $u$ and $v$ and re-run the plot. Compute by hand:\n",
    "- $u\\cdot v$\n",
    "- $\\|u\\|$, $\\|v\\|$\n",
    "- the projection coefficient $t=\\dfrac{u\\cdot v}{\\|v\\|^2}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set your own vectors, then run\n",
    "u = np.array([1.0, 3.0])\n",
    "v = np.array([2.0, -0.5])\n",
    "plot_vectors_and_projection(u, v, \" (your turn)\")\n",
    "print(\"u·v =\", float(u@v))\n",
    "print(\"||u|| =\", float(np.linalg.norm(u)), \"||v|| =\", float(np.linalg.norm(v)))\n",
    "print(\"t =\", float((u@v)/(v@v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f51741b",
   "metadata": {},
   "source": [
    "## 2) Visualizing distances\n",
    "\n",
    "For two points $a,b\\in\\mathbb{R}^2$, the Euclidean distance is\n",
    "$$\n",
    "\\|a-b\\| = \\sqrt{(a_1-b_1)^2 + (a_2-b_2)^2} = \\sqrt{\\|a\\|^2 + \\|b\\|^2 - 2a\\cdot b}.\n",
    "$$\n",
    "The plot shows the segment between two points and annotates its length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1e9daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_points_and_distance(a, b, title=\"Distance between two points\"):\n",
    "    plt.figure()\n",
    "    plt.scatter([a[0], b[0]], [a[1], b[1]])\n",
    "    plt.plot([a[0], b[0]], [a[1], b[1]])\n",
    "    d = np.linalg.norm(a - b)\n",
    "    mid = (a + b) / 2\n",
    "    plt.text(mid[0], mid[1], f\"d={d:.2f}\")\n",
    "    plt.axhline(0, linewidth=0.5); plt.axvline(0, linewidth=0.5)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.xlim(-5, 5); plt.ylim(-5, 5)\n",
    "    plt.grid(True, linewidth=0.3)\n",
    "    plt.xlabel(\"x\"); plt.ylabel(\"y\")\n",
    "    plt.title(title)\n",
    "\n",
    "a = np.array([2.0, -1.0])\n",
    "b = np.array([1.0, 3.0])\n",
    "plot_points_and_distance(a, b, \"Visualizing Euclidean distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1710695",
   "metadata": {},
   "source": [
    "### EXERCISE B — Pairwise distances and dot products\n",
    "For the set $P=\\{(1,0),(0,2),(-1,1)\\}$, compute a $3\\times3$ table of dot products $(p_i\\cdot p_j)$\n",
    "and another of squared distances $\\|p_i-p_j\\|^2$. (You can code it below or do by hand.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b67f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in P and compute pairwise dot products and squared distances\n",
    "import numpy as np\n",
    "\n",
    "P = np.array([[1.0, 0.0],\n",
    "              [0.0, 2.0],\n",
    "              [-1.0, 1.0]])\n",
    "\n",
    "def pairwise_dot(X):\n",
    "    return X @ X.T\n",
    "\n",
    "def pairwise_sqdist(X):\n",
    "    G = pairwise_dot(X)\n",
    "    diag = np.diag(G)\n",
    "    return diag[:,None] + diag[None,:] - 2*G\n",
    "\n",
    "dots = pairwise_dot(P)\n",
    "d2 = pairwise_sqdist(P)\n",
    "print(\"Pairwise dot products:\\n\", dots)\n",
    "print(\"\\nPairwise squared distances:\\n\", d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2089f553",
   "metadata": {},
   "source": [
    "## 3) Linear SVM on a simple 2D dataset\n",
    "\n",
    "We generate two Gaussian blobs (labels $\\pm1$), fit a linear SVM, and plot the **decision boundary**\n",
    "$w^\\top x + b=0$ and the **margins** $w^\\top x + b=\\pm 1$ (in original coordinates).\n",
    "The points marked with an 'x' are the **support vectors**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c2446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "def make_toy_blobs(n_per_class=40, separation=2.5):\n",
    "    cov = np.array([[1.0, 0.4],[0.4, 1.0]])\n",
    "    mean_pos = np.array([ separation,  separation])\n",
    "    mean_neg = np.array([-separation, -separation])\n",
    "    X_pos = np.random.multivariate_normal(mean_pos, cov, size=n_per_class)\n",
    "    X_neg = np.random.multivariate_normal(mean_neg, cov, size=n_per_class)\n",
    "    X = np.vstack([X_pos, X_neg])\n",
    "    y = np.hstack([np.ones(n_per_class), -np.ones(n_per_class)])\n",
    "    return X, y\n",
    "\n",
    "def plot_linear_svm_2d(X, y, C=1.0):\n",
    "    clf = Pipeline([(\"scaler\", StandardScaler()), (\"svm\", SVC(kernel=\"linear\", C=C))])\n",
    "    clf.fit(X, y)\n",
    "    svc = clf.named_steps[\"svm\"]; scaler = clf.named_steps[\"scaler\"]\n",
    "    w_z = svc.coef_.ravel(); b_z = float(svc.intercept_[0])\n",
    "    scale = scaler.scale_; mean = scaler.mean_\n",
    "    w_x = w_z / scale; b_x = b_z - np.dot(w_z, mean/scale)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(X[y==1,0], X[y==1,1], label=\"+1\")\n",
    "    plt.scatter(X[y==-1,0], X[y==-1,1], label=\"-1\")\n",
    "    if abs(w_x[1]) > 1e-12:\n",
    "        xs = np.linspace(X[:,0].min()-1, X[:,0].max()+1, 200)\n",
    "        db  = -(w_x[0]/w_x[1])*xs - b_x/w_x[1]\n",
    "        m1  = -(w_x[0]/w_x[1])*xs - (b_x+1)/w_x[1]\n",
    "        m_1 = -(w_x[0]/w_x[1])*xs - (b_x-1)/w_x[1]\n",
    "        plt.plot(xs, db); plt.plot(xs, m1); plt.plot(xs, m_1)\n",
    "    SV_z = svc.support_vectors_; SV_x = SV_z * scale + mean\n",
    "    plt.scatter(SV_x[:,0], SV_x[:,1], s=120, marker='x', label=\"support vectors\")\n",
    "    plt.legend(); plt.axhline(0, linewidth=0.5); plt.axvline(0, linewidth=0.5)\n",
    "    plt.gca().set_aspect('equal', adjustable='box'); plt.grid(True, linewidth=0.3)\n",
    "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "    plt.title(f\"Linear SVM on toy data (C={C})\")\n",
    "\n",
    "X, y = make_toy_blobs()\n",
    "plot_linear_svm_2d(X, y, C=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e92c684",
   "metadata": {},
   "source": [
    "### EXERCISE C — Change the regularization and measure hinge loss\n",
    "1. Re-run the previous cell with different values of $C$ (e.g., $0.1, 1, 10$). Describe how the margin/support vectors change.\n",
    "2. Compute the hinge losses $\\max(0, 1 - y_i (w^\\top x_i + b))$ for all points and report the average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12e0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute hinge losses for the current fitted model.\n",
    "# Hint: after fitting the Pipeline 'clf', you can recover w_x and b_x as in the code above,\n",
    "# then compute margins m_i = y_i * (X_i dot w_x + b_x).\n",
    "\n",
    "# Example skeleton:\n",
    "# m = y * (X @ w_x + b_x)\n",
    "# hinge = np.maximum(0.0, 1.0 - m)\n",
    "# print(\"Average hinge loss:\", hinge.mean())\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76244b8",
   "metadata": {},
   "source": [
    "## 4) Two-digit classification: Linear SVM vs LDA\n",
    "\n",
    "We try to use **MNIST** (digits 3 vs 8). If MNIST is unavailable in this environment (no internet/cache),\n",
    "we fall back to scikit-learn's 8×8 digits dataset (also digits 3 vs 8).\n",
    "\n",
    "Both models are trained on flattened pixel features after scaling to $[0,1]$ and standardized per feature.\n",
    "We report held-out test accuracy and show confusion matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b64600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.datasets import load_digits as sklearn_load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def try_load_mnist_two_digits(d1=3, d2=8, limit_per_class=150):\n",
    "    try:\n",
    "        from tensorflow.keras.datasets import mnist\n",
    "        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "        train_mask = np.isin(y_train, [d1, d2]); test_mask = np.isin(y_test, [d1, d2])\n",
    "        X_train = X_train[train_mask]; y_train = y_train[train_mask]\n",
    "        X_test = X_test[test_mask];   y_test  = y_test[test_mask]\n",
    "        if limit_per_class is not None:\n",
    "            def limit(X, y, digit, k):\n",
    "                idx = np.where(y==digit)[0][:k]\n",
    "                return X[idx], y[idx]\n",
    "            X1, y1 = limit(X_train, y_train, d1, limit_per_class)\n",
    "            X2, y2 = limit(X_train, y_train, d2, limit_per_class)\n",
    "            X_train = np.vstack([X1, X2]); y_train = np.concatenate([y1, y2])\n",
    "            X1t, y1t = limit(X_test, y_test, d1, max(200, limit_per_class))\n",
    "            X2t, y2t = limit(X_test, y_test, d2, max(200, limit_per_class))\n",
    "            X_test = np.vstack([X1t, X2t]); y_test = np.concatenate([y1t, y2t])\n",
    "        X_train = X_train.reshape(len(X_train), -1) / 255.0\n",
    "        X_test  = X_test.reshape(len(X_test),  -1) / 255.0\n",
    "        y_train = np.where(y_train==d1, 0, 1); y_test = np.where(y_test==d1, 0, 1)\n",
    "        return (\"MNIST\", (X_train, X_test, y_train, y_test))\n",
    "    except Exception:\n",
    "        digits = sklearn_load_digits()\n",
    "        X = digits.data / 16.0; y = digits.target\n",
    "        mask = np.isin(y, [d1, d2]); X = X[mask]; y = y[mask]\n",
    "        y = np.where(y==d1, 0, 1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "        return (\"sklearn_digits_8x8\", (X_train, X_test, y_train, y_test))\n",
    "\n",
    "def evaluate_linear_svm_vs_lda(X_train, X_test, y_train, y_test):\n",
    "    svm_pipe = Pipeline([(\"scaler\", StandardScaler(with_mean=True)), (\"svm\", SVC(kernel=\"linear\", C=1.0))])\n",
    "    svm_pipe.fit(X_train, y_train)\n",
    "    yhat_svm = svm_pipe.predict(X_test)\n",
    "    acc_svm = accuracy_score(y_test, yhat_svm)\n",
    "    cm_svm = confusion_matrix(y_test, yhat_svm)\n",
    "\n",
    "    lda_pipe = Pipeline([(\"scaler\", StandardScaler(with_mean=True)), (\"lda\", LDA())])\n",
    "    lda_pipe.fit(X_train, y_train)\n",
    "    yhat_lda = lda_pipe.predict(X_test)\n",
    "    acc_lda = accuracy_score(y_test, yhat_lda)\n",
    "    cm_lda = confusion_matrix(y_test, yhat_lda)\n",
    "    return (acc_svm, cm_svm), (acc_lda, cm_lda)\n",
    "\n",
    "name, (Xtr, Xte, ytr, yte) = try_load_mnist_two_digits()\n",
    "(acc_svm, cm_svm), (acc_lda, cm_lda) = evaluate_linear_svm_vs_lda(Xtr, Xte, ytr, yte)\n",
    "print(\"Dataset used:\", name)\n",
    "print(\"Linear SVM accuracy:\", acc_svm)\n",
    "print(\"LDA accuracy:\", acc_lda)\n",
    "\n",
    "plt.figure(); plt.imshow(cm_svm, interpolation='nearest'); plt.title(f\"SVM Confusion ({name})\"); plt.colorbar(); plt.xlabel(\"Pred\"); plt.ylabel(\"True\")\n",
    "plt.figure(); plt.imshow(cm_lda, interpolation='nearest'); plt.title(f\"LDA Confusion ({name})\"); plt.colorbar(); plt.xlabel(\"Pred\"); plt.ylabel(\"True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55901e",
   "metadata": {},
   "source": [
    "### EXERCISE D — Explore digits and regularization\n",
    "- Change the two digits (e.g., 1 vs 7) and re-run.  \n",
    "- Try different $C$ values for the SVM.  \n",
    "- (Optional) Compare using/omitting standardization before LDA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba4a3c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### What to notice\n",
    "- Dot products drive both **projections** and the **score** $w^\\top x + b$.\n",
    "- Distance to a line is just a **scaled score**: $\\frac{|w^\\top x + b|}{\\|w\\|}$.\n",
    "- For the toy data, the **support vectors** are the points that touch the margin lines $\\pm 1$.\n",
    "- On digits, a linear SVM and LDA can perform differently; compare accuracies and confusion matrices.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
