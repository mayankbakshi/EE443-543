{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f5689d6",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis on MNIST\n",
    "\n",
    "This notebook is a first look at handwritten digit classification for students beginning their statistical learning journey. Following the Gaussian perspective from *An Introduction to Statistical Learning with Python* (ISLP), we will:\n",
    "\n",
    "- load and preview the MNIST digits,\n",
    "- estimate the class means and a common covariance matrix,\n",
    "- plug those estimates into the LDA decision rule,\n",
    "- evaluate the resulting classifier on training and test data, and\n",
    "- inspect individual predictions to build intuition.\n",
    "\n",
    "Work through the cells in order; the commentary ties each computation back to the formulas you encounter in lecture.\n",
    "\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mayankbakshi/EE443-543/blob/main/Classification.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a72546",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "- Python with `numpy`, `matplotlib`, and `scikit-learn`\n",
    "- Optional: `ISLP` for a textbook-aligned LDA interface (`pip install islp`)\n",
    "- The first run needs internet access to download MNIST; afterwards the data is cached in `./data/mnist_784.npz` (or `/content/data` on Google Colab)\n",
    "\n",
    "If you are on Colab, run the next cell to install any missing packages before moving on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc845a",
   "metadata": {
    "tags": [
     "colab-setup"
    ]
   },
   "outputs": [],
   "source": [
    "# Install optional dependencies (safe to rerun)\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def ensure_package(module_name: str, package_name: Optional[str] = None) -> None:\n",
    "    try:\n",
    "        importlib.import_module(module_name)\n",
    "        print(f\"{module_name} is already installed.\")\n",
    "    except ImportError:\n",
    "        pkg = package_name or module_name\n",
    "        print(f\"Installing {pkg} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "        importlib.import_module(module_name)\n",
    "        print(f\"{module_name} installation complete.\")\n",
    "\n",
    "\n",
    "ensure_package(\"ISLP\", \"islp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651199fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd63f6",
   "metadata": {},
   "source": [
    "## Load and cache MNIST\n",
    "\n",
    "The next cell pulls MNIST from OpenML on the first run and stores a compressed copy locally to avoid repeated downloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5741a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a folder to store MNIST so repeated runs can reuse the cached file\n",
    "colab_root = Path(\"/content\") if \"google.colab\" in sys.modules else Path.cwd()\n",
    "DATA_PATH = colab_root / \"data\"\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "cache_file = DATA_PATH / \"mnist_784.npz\"\n",
    "\n",
    "# If we already saved the dataset locally, load it back from disk\n",
    "if cache_file.exists():\n",
    "    cached = np.load(cache_file, allow_pickle=False)\n",
    "    X = cached[\"X\"]\n",
    "    y = cached[\"y\"]\n",
    "    print(\"Loaded cached MNIST from\", cache_file)\n",
    "else:\n",
    "    # First run: download MNIST from OpenML (images arrive as 28×28=784 pixel vectors)\n",
    "    mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "    X = mnist[\"data\"].astype(np.float32)\n",
    "    y = mnist[\"target\"].astype(int)\n",
    "    np.savez_compressed(cache_file, X=X, y=y)\n",
    "    print(\"Downloaded MNIST and cached at\", cache_file)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")  # (number of images, pixels per image)\n",
    "print(f\"Label vector shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f93c57",
   "metadata": {},
   "source": [
    "## Quick look at the digits\n",
    "\n",
    "A quick visualization helps connect the raw pixel grid to the numbers we expect the algorithm to learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed907d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a few sample digits to connect the data array back to handwriting\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for ax, image, label in zip(axes.ravel(), X[:10], y[:10]):\n",
    "    # MNIST stores each image as a flat 784-length vector; reshape to 28×28 for display\n",
    "    ax.imshow(image.reshape(28, 28), cmap=\"gray\")\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e25d3",
   "metadata": {},
   "source": [
    "## Background: Gaussian view of the digits\n",
    "\n",
    "LDA treats each class (digit) as if it were drawn from a multivariate Normal distribution with mean vector $\\mu_k$ and a shared covariance matrix $\\Sigma$. The Gaussian assumption delivers simple, interpretable formulas for the class summaries you are about to compute. Keep these guiding questions in mind:\n",
    "\n",
    "1. What patterns appear when we average the handwriting for each digit?\n",
    "2. Can one covariance matrix describe the within-class variation for all digits?\n",
    "3. How does the Gaussian model turn these summaries into a decision boundary?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de289d3",
   "metadata": {},
   "source": [
    "## Split data and optionally down-sample for faster experimentation\n",
    "\n",
    "To evaluate a learning method fairly, we create two groups of examples:\n",
    "- a **training set** that the model sees while it learns, and\n",
    "- a **test set** that stays hidden until the very end.\n",
    "\n",
    "MNIST has 60,000 training images. LDA needs to invert a large covariance matrix, so we take a random subset (20,000 images) of the training set to keep the runtime manageable in a classroom setting. Feel free to increase this cap when you have more compute time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de8ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=10000, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "max_train_samples = 20000\n",
    "if X_train.shape[0] > max_train_samples:\n",
    "    rng = np.random.default_rng(0)\n",
    "    keep_idx = rng.choice(X_train.shape[0], max_train_samples, replace=False)\n",
    "    X_train_work = X_train[keep_idx]\n",
    "    y_train_work = y_train[keep_idx]\n",
    "else:\n",
    "    X_train_work = X_train\n",
    "    y_train_work = y_train\n",
    "\n",
    "print(f\"Training on {X_train_work.shape[0]} samples, testing on {X_test.shape[0]}\")\n",
    "# Each row still represents a 28×28 image flattened into 784 numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df8837a",
   "metadata": {},
   "source": [
    "## Mean of each digit\n",
    "\n",
    "Under the Gaussian model, the sample mean\n",
    "\\[\n",
    "\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i: y_i = k} x_i\n",
    "\\]\n",
    "serves as our estimate of the true class mean. The cell below computes these averages and visualizes the typical appearance of each digit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76060414",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = np.unique(y_train_work)\n",
    "mean_vectors = {}\n",
    "class_counts = {}\n",
    "for digit in digits:\n",
    "    digit_pixels = X_train_work[y_train_work == digit].astype(np.float64)\n",
    "    class_counts[digit] = digit_pixels.shape[0]\n",
    "    # Store the average pixel pattern for this digit (length 784)\n",
    "    mean_vectors[digit] = digit_pixels.mean(axis=0)\n",
    "\n",
    "mean_matrix = np.vstack([mean_vectors[d] for d in digits])\n",
    "mean_intensity = np.array([mean_vectors[d].mean() for d in digits])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].bar(digits, mean_intensity)\n",
    "axes[0].set_title(\"Mean pixel intensity by digit\")\n",
    "axes[0].set_xlabel(\"Digit\")\n",
    "axes[0].set_ylabel(\"Mean intensity\")\n",
    "\n",
    "axes[1].bar(digits, [class_counts[d] for d in digits], color=\"orange\")\n",
    "axes[1].set_title(\"Training samples per digit\")\n",
    "axes[1].set_xlabel(\"Digit\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for ax, digit in zip(axes.ravel(), digits):\n",
    "    mean_image = mean_vectors[digit].reshape(28, 28)  # reshape back to image form\n",
    "    ax.imshow(mean_image, cmap=\"gray\")\n",
    "    ax.set_title(f\"Mean of {digit}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0ebf4",
   "metadata": {},
   "source": [
    "## Common covariance estimate\n",
    "\n",
    "LDA assumes all digits share the same covariance matrix. The pooled estimator from ISLP is\n",
    "\\[\n",
    "\\hat{\\Sigma} = \\frac{1}{N - K} \\sum_{k=0}^{K-1} \\sum_{i: y_i = k} (x_i - \\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^\\top,\n",
    "\\]\n",
    "where $N$ is the number of training examples we kept and $K$ is the number of classes. This captures the average within-class spread of the handwriting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c96e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = X_train_work.shape[1]\n",
    "pooled_cov = np.zeros((feature_dim, feature_dim), dtype=np.float64)\n",
    "for digit in digits:\n",
    "    digit_pixels = X_train_work[y_train_work == digit].astype(np.float64)\n",
    "    centered = digit_pixels - mean_vectors[digit]  # subtract the class mean (centering step)\n",
    "    pooled_cov += centered.T @ centered\n",
    "pooled_cov /= (X_train_work.shape[0] - len(digits))\n",
    "\n",
    "print(\"Pooled covariance matrix shape:\", pooled_cov.shape)\n",
    "print(\"Average variance (diagonal mean):\", np.mean(np.diag(pooled_cov)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12713b98",
   "metadata": {},
   "source": [
    "## LDA decision rule\n",
    "\n",
    "ISLP derives the linear discriminant score for class $k$ as\n",
    "\\[\n",
    "\\delta_k(x) = x^\\top \\hat{\\Sigma}^{-1} \\hat{\\mu}_k - \\tfrac{1}{2} \\hat{\\mu}_k^\\top \\hat{\\Sigma}^{-1} \\hat{\\mu}_k + \\log \\hat{\\pi}_k,\n",
    "\\]\n",
    "where $\\hat{\\pi}_k = n_k / N$ is the empirical class prior. We classify a new image by choosing the digit with the largest discriminant score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae67f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical class prior probabilities π_k (ISLP notation)\n",
    "prior_counts = np.array([class_counts[d] for d in digits], dtype=np.float64)\n",
    "priors = prior_counts / prior_counts.sum()\n",
    "\n",
    "# β_k = Σ^{-1} μ_k gives the slope vectors in the discriminant score\n",
    "Sigma_inv = np.linalg.pinv(pooled_cov)\n",
    "beta_hat = (Sigma_inv @ mean_matrix.T).T\n",
    "\n",
    "# β_{k0} = -½ μ_k^T Σ^{-1} μ_k + log π_k is the intercept term\n",
    "temp = np.sum(mean_matrix * beta_hat, axis=1)\n",
    "beta0_hat = -0.5 * temp + np.log(priors)\n",
    "\n",
    "def lda_discriminant(X):\n",
    "    \"\"\"Return δ_k(x) for every observation and class (ISLP Eq. 4.14).\"\"\"\n",
    "    return X @ beta_hat.T + beta0_hat\n",
    "\n",
    "\n",
    "train_scores = lda_discriminant(X_train_work)\n",
    "y_pred_train = digits[np.argmax(train_scores, axis=1)]\n",
    "\n",
    "test_scores = lda_discriminant(X_test)\n",
    "y_pred_test = digits[np.argmax(test_scores, axis=1)]\n",
    "\n",
    "train_accuracy = accuracy_score(y_train_work, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy:.2%} | Train error: {(1 - train_accuracy):.2%}\")\n",
    "print(f\"Test accuracy:  {test_accuracy:.2%} | Test error:  {(1 - test_accuracy):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a79c8b",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "A confusion matrix shows, for each true digit, how many examples were predicted as every possible digit. Look for darker diagonal squares (good) and off-diagonal squares (misclassifications worth investigating).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e36b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ConfusionMatrixDisplay(cm).plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "ax.set_title(\"LDA confusion matrix on MNIST\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6549784",
   "metadata": {},
   "source": [
    "## Inference examples\n",
    "\n",
    "Numbers alone can hide systematic errors. Sampling a few test digits helps us see whether the classifier confuses similar shapes and how confident it appears.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245bbdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 8\n",
    "rng = np.random.default_rng(2024)\n",
    "example_idx = rng.choice(X_test.shape[0], size=num_examples, replace=False)\n",
    "example_images = X_test[example_idx]\n",
    "example_true = y_test[example_idx]\n",
    "example_pred = y_pred_test[example_idx]\n",
    "\n",
    "# Score gap: δ̂_pred(x) - δ̂_second(x) (positive values imply a clear winner)\n",
    "example_scores = test_scores[example_idx]\n",
    "top_scores = example_scores.max(axis=1)\n",
    "second_scores = np.partition(example_scores, -2, axis=1)[:, -2]\n",
    "score_gaps = top_scores - second_scores\n",
    "\n",
    "fig, axes = plt.subplots(2, num_examples // 2, figsize=(12, 5))\n",
    "for ax, image, true_label, pred_label, gap in zip(\n",
    "    axes.ravel(), example_images, example_true, example_pred, score_gaps\n",
    "):\n",
    "    ax.imshow(image.reshape(28, 28), cmap=\"gray\")  # reshape flat vector for display\n",
    "    title = f\"pred {pred_label}\\ntrue {true_label}\\nscore gap {gap:.2f}\"\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ee4acf",
   "metadata": {},
   "source": [
    "## Logistic regression baseline\n",
    "\n",
    "To compare against a discriminative classifier, we fit a multinomial logistic regression model using scikit-learn's `LogisticRegression`. ISLP (Chapter 4) shows that maximizing the multinomial log-likelihood yields class probabilities\n",
    "\\[\n",
    "P(Y = k \\mid x) = \\frac{\\exp(\\eta_{k0} + x^\\top \\eta_k)}{\\sum_{\\ell=0}^{K-1} \\exp(\\eta_{\\ell0} + x^\\top \\eta_\\ell)}.\n",
    "\\]\n",
    "\n",
    "We standardize the pixels and fit the model without regularization so the result matches the textbook derivation as closely as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "log_reg_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        solver='lbfgs',\n",
    "        penalty='none',\n",
    "        max_iter=200,\n",
    "        n_jobs=-1,\n",
    "        random_state=0,\n",
    "    ))\n",
    "])\n",
    "\n",
    "log_reg_pipeline.fit(X_train_work, y_train_work)\n",
    "\n",
    "logreg_train_pred = log_reg_pipeline.predict(X_train_work)\n",
    "logreg_test_pred = log_reg_pipeline.predict(X_test)\n",
    "\n",
    "logreg_train_acc = accuracy_score(y_train_work, logreg_train_pred)\n",
    "logreg_test_acc = accuracy_score(y_test, logreg_test_pred)\n",
    "\n",
    "print(f\"Logistic regression train accuracy: {logreg_train_acc:.2%}\")\n",
    "print(f\"Logistic regression test accuracy:  {logreg_test_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbeedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_cm = confusion_matrix(y_test, logreg_test_pred)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ConfusionMatrixDisplay(logreg_cm).plot(ax=ax, cmap='Greens', colorbar=False)\n",
    "ax.set_title('Logistic regression confusion matrix on MNIST')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c29ac1f",
   "metadata": {},
   "source": [
    "## Exercise 1: Discuss MNIST\n",
    "- Compare the training vs. test accuracy and comment on whether logistic regression appears to overfit.\n",
    "- Using the LDA confusion matrix, quantify the false positives and false negatives between digits 4 and 9. What handwriting features do you suspect drive those mistakes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff610c8",
   "metadata": {},
   "source": [
    "## Exercise 2: Apply LDA to Fashion-MNIST\n",
    "\n",
    "This optional practice extends the Gaussian-LDA workflow to the Fashion-MNIST clothing dataset. The images have the same 28×28 layout as MNIST but depict shirts, shoes, and other items, so the class structure is different.\n",
    "\n",
    "**Your tasks:**\n",
    "1. Run the setup cell to download (or reuse) Fashion-MNIST and create a manageable train/test split.\n",
    "2. Complete the code cells labelled **TODO** to estimate the class means, the pooled covariance matrix, and the LDA decision rule.\n",
    "3. Answer the reflection questions using the plots and metrics you generate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97e36f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Fetch Fashion-MNIST and prepare a train/test split (run this cell once)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "FASHION_CACHE = DATA_PATH / \"fashion_mnist.npz\"\n",
    "\n",
    "if FASHION_CACHE.exists():\n",
    "    cached = np.load(FASHION_CACHE, allow_pickle=False)\n",
    "    X_fashion = cached[\"X\"]\n",
    "    y_fashion = cached[\"y\"]\n",
    "    print(\"Loaded cached Fashion-MNIST from\", FASHION_CACHE)\n",
    "else:\n",
    "    # Download Fashion-MNIST from OpenML. Each image is again a 28×28 grayscale grid.\n",
    "    fashion = fetch_openml(\"fashion-mnist\", version=1, as_frame=False)\n",
    "    X_fashion = fashion[\"data\"].astype(np.float32)\n",
    "    y_fashion = fashion[\"target\"].astype(int)\n",
    "    np.savez_compressed(FASHION_CACHE, X=X_fashion, y=y_fashion)\n",
    "    print(\"Downloaded Fashion-MNIST and cached at\", FASHION_CACHE)\n",
    "\n",
    "X_fashion_train, X_fashion_test, y_fashion_train, y_fashion_test = train_test_split(\n",
    "    X_fashion, y_fashion, test_size=10000, stratify=y_fashion, random_state=1\n",
    ")\n",
    "\n",
    "max_train_samples = 20000\n",
    "if X_fashion_train.shape[0] > max_train_samples:\n",
    "    rng = np.random.default_rng(1)\n",
    "    keep_idx = rng.choice(X_fashion_train.shape[0], max_train_samples, replace=False)\n",
    "    X_fashion_train = X_fashion_train[keep_idx]\n",
    "    y_fashion_train = y_fashion_train[keep_idx]\n",
    "\n",
    "print(f\"Fashion-MNIST train: {X_fashion_train.shape}, test: {X_fashion_test.shape}\")\n",
    "print(\"Pixel values range:\", X_fashion_train.min(), \"to\", X_fashion_train.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e9ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: Estimate class means for Fashion-MNIST\n",
    "fashion_digits = np.unique(y_fashion_train)\n",
    "fashion_counts = {}\n",
    "fashion_mean_vectors = {}\n",
    "\n",
    "for digit in fashion_digits:\n",
    "    class_pixels = X_fashion_train[y_fashion_train == digit].astype(np.float64)\n",
    "    fashion_counts[digit] = class_pixels.shape[0]\n",
    "    # TODO: replace the next line with the class mean (shape: 784, one value per pixel)\n",
    "    raise NotImplementedError(\"Store the mean vector for this Fashion-MNIST class before moving on.\")\n",
    "\n",
    "# TODO: assemble the class means into a matrix (same order as fashion_digits)\n",
    "raise NotImplementedError(\"Create fashion_mean_matrix = np.vstack([...]) just like in the MNIST section.\")\n",
    "\n",
    "print(\"Once you finish this cell, you can visualize the means with reshape(28, 28) just like before.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3631aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: Build the pooled covariance estimate for Fashion-MNIST\n",
    "feature_dim = X_fashion_train.shape[1]\n",
    "fashion_pooled_cov = np.zeros((feature_dim, feature_dim), dtype=np.float64)\n",
    "\n",
    "for digit in fashion_digits:\n",
    "    class_pixels = X_fashion_train[y_fashion_train == digit].astype(np.float64)\n",
    "    centered = class_pixels - fashion_mean_vectors[digit]  # requires TODO 1\n",
    "    # TODO: accumulate (x - mean)(x - mean)^T for this class\n",
    "    raise NotImplementedError(\"Add the class contribution to fashion_pooled_cov.\")\n",
    "\n",
    "# TODO: divide by (N - K) to finish the pooled covariance (match the ISLP formula)\n",
    "raise NotImplementedError(\"Finish the fashion_pooled_cov scaling step.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66963a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3: Evaluate your Fashion-MNIST LDA classifier\n",
    "fashion_prior_counts = np.array([fashion_counts[d] for d in fashion_digits], dtype=np.float64)\n",
    "# TODO: convert counts to probabilities π_k\n",
    "raise NotImplementedError(\"Compute fashion_priors using fashion_prior_counts.\")\n",
    "\n",
    "# TODO: compute β_k = Σ^{-1} μ_k and β_{k0} = -½ μ_k^T Σ^{-1} μ_k + log π_k\n",
    "raise NotImplementedError(\"Compute fashion_beta and fashion_beta0 using your pooled covariance and means.\")\n",
    "\n",
    "# TODO: write a discriminant function that returns δ̂_k(x) for every observation\n",
    "raise NotImplementedError(\"Define fashion_discriminant(X) using fashion_beta and fashion_beta0.\")\n",
    "\n",
    "# TODO: score the training and test sets, then convert scores to predicted classes\n",
    "raise NotImplementedError(\"Produce Fashion-MNIST train/test predictions with argmax.\")\n",
    "\n",
    "# TODO: compute and print accuracy (and optionally a confusion matrix) to summarise performance\n",
    "raise NotImplementedError(\"Report the train/test accuracy for Fashion-MNIST.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cadc2d5",
   "metadata": {},
   "source": [
    "## K-means clustering on MNIST\n",
    "\n",
    "To connect with ISLP's unsupervised-learning chapter, we implement the K-means algorithm directly: initialize centroids, assign points to the nearest centroid, and update those centroids until convergence. This illustrates the core geometry of minimizing within-cluster sums of squares without relying on scikit-learn's `KMeans` helper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "rng_kmeans = np.random.default_rng(42)\n",
    "subset_size = 5000\n",
    "if X_train_work.shape[0] > subset_size:\n",
    "    subset_idx = rng_kmeans.choice(X_train_work.shape[0], subset_size, replace=False)\n",
    "    X_kmeans = X_train_work[subset_idx]\n",
    "else:\n",
    "    X_kmeans = X_train_work\n",
    "\n",
    "scaler_kmeans = StandardScaler()\n",
    "X_kmeans_scaled = scaler_kmeans.fit_transform(X_kmeans)\n",
    "\n",
    "\n",
    "def run_kmeans(X, k, max_iter=200, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_samples = X.shape[0]\n",
    "    indices = rng.choice(n_samples, size=k, replace=False)\n",
    "    centroids = X[indices]\n",
    "    labels = np.full(n_samples, -1, dtype=int)\n",
    "\n",
    "    for iteration in range(1, max_iter + 1):\n",
    "        distances = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)\n",
    "        new_labels = np.argmin(distances, axis=1)\n",
    "\n",
    "        if np.array_equal(new_labels, labels):\n",
    "            iteration_count = iteration - 1\n",
    "            break\n",
    "\n",
    "        labels = new_labels\n",
    "        new_centroids = centroids.copy()\n",
    "        for j in range(k):\n",
    "            members = X[labels == j]\n",
    "            if members.size == 0:\n",
    "                new_centroids[j] = X[rng.integers(0, n_samples)]\n",
    "            else:\n",
    "                new_centroids[j] = members.mean(axis=0)\n",
    "        centroids = new_centroids\n",
    "    else:\n",
    "        iteration_count = max_iter\n",
    "\n",
    "    inertia = np.sum((X - centroids[labels]) ** 2)\n",
    "\n",
    "    return {\n",
    "        'centroids': centroids,\n",
    "        'labels': labels,\n",
    "        'inertia': inertia,\n",
    "        'iterations': iteration_count,\n",
    "    }\n",
    "\n",
    "\n",
    "k_values = [3, 7, 10, 20]\n",
    "kmeans_runs = {}\n",
    "\n",
    "for k in k_values:\n",
    "    result = run_kmeans(X_kmeans_scaled, k, seed=k)\n",
    "    kmeans_runs[k] = result\n",
    "    print(f\"k={k:<2} inertia={result['inertia']:.1f}, iterations={result['iterations']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cb6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_random_examples(run, max_per_cluster=5, title_prefix=''):\n",
    "    labels = run['labels']\n",
    "    centers = scaler_kmeans.inverse_transform(run['centroids'])\n",
    "    unique_clusters = len(run['centroids'])\n",
    "    fig, axes = plt.subplots(unique_clusters, max_per_cluster + 1, figsize=(12, 2 * unique_clusters))\n",
    "    if unique_clusters == 1:\n",
    "        axes = axes[None, :]\n",
    "    for cluster_idx in range(unique_clusters):\n",
    "        axes[cluster_idx, 0].imshow(centers[cluster_idx].reshape(28, 28), cmap='gray')\n",
    "        axes[cluster_idx, 0].set_title(f\"{title_prefix} {cluster_idx} centroid\")\n",
    "        axes[cluster_idx, 0].axis('off')\n",
    "        members = np.where(labels == cluster_idx)[0]\n",
    "        if members.size > 0:\n",
    "            chosen = rng_kmeans.choice(members, size=min(max_per_cluster, members.size), replace=False)\n",
    "            for col_offset, idx_img in enumerate(chosen, start=1):\n",
    "                axes[cluster_idx, col_offset].imshow(X_kmeans[idx_img].reshape(28, 28), cmap='gray')\n",
    "                axes[cluster_idx, col_offset].axis('off')\n",
    "            start_empty = 1 + len(chosen)\n",
    "        else:\n",
    "            start_empty = 1\n",
    "        for col_offset in range(start_empty, max_per_cluster + 1):\n",
    "            axes[cluster_idx, col_offset].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "k_display = 10\n",
    "if k_display in kmeans_runs:\n",
    "    display_random_examples(kmeans_runs[k_display], title_prefix='k=10 cluster')\n",
    "else:\n",
    "    print(f'Choose a display k that is in {list(kmeans_runs)}')\n",
    "\n",
    "for k_show in [3, 7, 20]:\n",
    "    if k_show in kmeans_runs:\n",
    "        print(f'Random samples from clusters for k={k_show}')\n",
    "        display_random_examples(kmeans_runs[k_show], title_prefix=f'k={k_show}')\n",
    "    else:\n",
    "        print(f'k={k_show} not available; choose from {list(kmeans_runs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c79868",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, run in kmeans_runs.items():\n",
    "    counts = np.bincount(run['labels'], minlength=k)\n",
    "    print(f'k={k:<2} cluster sizes: {counts}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4606906d",
   "metadata": {},
   "source": [
    "### K-means reflection prompts\n",
    "- Compare the centroids for $k=10$ with the class means from the supervised section. Which digits are captured cleanly, and which mix together?\n",
    "- Inspect the inertia values: does increasing $k$ beyond 10 provide meaningful gains given the extra complexity?\n",
    "- Try projecting the clustered points into two discriminant directions or using PCA (as in ISLP Chapter 10) to visualize cluster separation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17f879",
   "metadata": {},
   "source": [
    "### Reflection questions\n",
    "- Which Fashion-MNIST classes share the most similar mean images? Why might that make classification harder?\n",
    "- Compare the largest confusion pairs in Fashion-MNIST against MNIST. Does the pooled covariance seem to capture clothing variability as well as handwriting variability?\n",
    "- If you wanted to improve accuracy, which assumption of LDA would you relax first? (Different covariances? Non-linear boundaries?) Explain your reasoning briefly.\n",
    "\n",
    "> *Tip:* Save your answers in the notebook or discuss them in your write-up so you can revisit the reasoning when you study for exams.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b3b65e",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "- Estimating the Gaussian parameters directly from the data gives transparent building blocks for LDA: sample means, a pooled covariance, and empirical class priors.\n",
    "- Plugging those estimates into the ISLP discriminant formula yields a linear decision rule that performs well on MNIST, even with a reduced training set.\n",
    "- Visual diagnostics—average digits, confusion matrices, and sampled predictions—make it easier to explain why the classifier succeeds or fails.\n",
    "- Next steps: compare against logistic regression, experiment with more training examples, or explore quadratic discriminant analysis (QDA) where each digit gets its own covariance matrix.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
